---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
urlcolor: blue
---

You can also find publications on my <span style="color:blue">[Google Scholar](https://scholar.google.com/citations?user=SAnJ1hIAAAAJ&hl=en")</span> profile. 


**Deconstructing Distributions: A Pointwise Framework of Learning**  
Gal Kapluni\*, Nikhil Ghosh\*, Saurabh Garg, Boaz Barak, Preetum Nakkiran  
Under Submission   



**Leveraging Unlabeled Data to Predict Out-of-Distribution Performance**   
Saurabh Garg, Sivaraman Balakrishnan, Zachary Lipton, Behnam Neyshabur, Hanie Sedghi  
NeurIPS Workshop on Distribution Shift (DistShift), 2021   
International Conference on Learning Representations (ICLR), 2022   
<span style="color:blue">[Paper](https://arxiv.org/abs/2201.04234)</span> / <span style="color:blue">[Code]()</span> / <span style="color:blue">[Talk]()</span> / <span style="color:blue">[Poster]()</span> / <span style="color:blue">[Summary](javascript:toggleblock('garg2022_ATC_abs'))</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2022_ATC_bib'))</span>
<p><i id="garg2022_ATC_abs" style="display: none;"> </i></p>
<bibtext xml:space="preserve" id="garg2022_ATC_bib" style="display: none;">
@article{garg2022ATC, <br>
 title={Leveraging Unlabeled Data to Predict Out-of-Distribution Performance} <br>
 author={Garg, Saurabh and Balakrishnan, Sivaraman and Lipton, Zachary and Neyshabur, Behnam and Sedghi, Hanie}, <br>
 year={2022}, <br>
 journal={arXiv preprint arXiv:2201.04234} <br>
 url={https://arxiv.org/abs/2201.04234}, <br>
 note={ICLR 2022}<br>
}
</bibtext>

**Mixture Proportion Estimation and PU Learning: A Modern Approach**  
Saurabh Garg, Yifan Wu, Alex Smola, Sivaraman Balakrishnan, Zachary Lipton  
<span style="color:red">Spotlight</span> at Advances in Neural Information Processing Systems (NeurIPS), 2021       
ICML Workshop on Uncertainty in Deep Learning, 2021   
<span style="color:blue">[Paper](https://arxiv.org/abs/2111.00980)</span> / <span style="color:blue">[Code](https://github.com/acmi-lab/PU_learning)</span> / <span style="color:blue">[Talk](https://recorder-v3.slideslive.com/?share=52493&s=dafe9010-00e1-4082-b224-0391b356ccbb)</span> / <span style="color:blue">[Poster](https://drive.google.com/file/d/1liowCWEHxMZH2Ag5ozaJfAG58RCwOh86/view?usp=sharing)</span> / <span style="color:blue">[Summary](javascript:toggleblock('garg2021_PU_learning_abs'))</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2021_PU_learning_bib'))</span>
<p><i id="garg2021_PU_learning_abs" style="display: none;">Given only Positive (P) and Unlabeled (U) data, containing both P and Negative (N) samples, we propose new approaches to estimate fraction of P in U and learn P vs N classifier. </i></p>
<bibtext xml:space="preserve" id="garg2021_PU_learning_bib" style="display: none;">
@article{garg2021PUlearning, <br> 
 title={Mixture Proportion Estimation and PU Learning: A Modern Approach} <br>
 author={Garg, Saurabh and Wu, Yifan and Smola, Alex and Balakrishnan, Sivaraman and Lipton, Zachary}, <br>
 year={2021}, <br>
 journal={arXiv preprint arXiv:2111.00980} <br>
 url={https://arxiv.org/abs/2111.00980}, <br>
 note={Spotlight at NeurIPS 2021}<br>
}
</bibtext>

**RATT: Leveraging Unlabeled Data to Guarantee Generalization**   
Saurabh Garg, Sivaraman Balakrishnan, Zico Kolter, Zachary Lipton   
<span style="color:red">Long Talk</span> at International Conference on Machine Learning (ICML), 2021   
ICLR Workshop on RobustML, 2021   
<span style="color:blue">[Paper](https://arxiv.org/abs/2105.00303)</span> / <span style="color:blue">[Code](https://github.com/acmi-lab/RATT_generalization_bound)</span> / <span style="color:blue">[Talk](https://slideslive.com/38958661/ratt-leveraging-unlabeled-data-to-guarantee-generalization?ref=speaker-37449-latest)</span> / <span style="color:blue">[Poster](https://drive.google.com/file/d/1H25csKq622EDMtw2en-aDQxqNcP1Mcdg/view?usp=sharing)</span> / <span style="color:blue">[Summary](javascript:toggleblock('garg2021_RATT_abs'))</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2021_RATT_bib'))</span>
<p><i id="garg2021_RATT_abs" style="display: none;">We introduce a method that leverages unlabeled data to produce generalization bound.  When a trained model fits clean training data well but randomly labeled training data added in poorly, we show that its generalization to the population is guaranteed.</i></p>
<bibtext xml:space="preserve" id="garg2021_RATT_bib" style="display: none;">
@article{garg2021RATT, <br>
 title={ {RATT}: Leveraging Unlabeled Data to Guarantee Generalization } <br>
 author={Garg, Saurabh and Balakrishnan, Sivaraman and Kolter, Zico and Lipton, Zachary}, <br>
 year={2021}, <br>
 journal={arXiv preprint arXiv:2105.00303} <br>
 url={https://arxiv.org/abs/2105.00303}, <br>
 note={Long Talk at ICML 2021}<br>
}
</bibtext>


**On Proximal Policy Optimization's Heavy-Tailed Gradients**   
Saurabh Garg, Joshua Zhanson, Emilio Parisotto, Adarsh Prasad, Zico Kolter, Zachary Lipton, Sivaraman Balakrishnan, Ruslan Salakhutdinov, Pradeep Ravikumar    
Short talk at International Conference on Machine Learning (ICML), 2021   
ICLR Workshop on Science and Engineering of Deep Learning, 2021    
<span style="color:blue">[Paper](https://arxiv.org/abs/2102.10264)</span> / <span style="color:blue">[Code]()</span> / <span style="color:blue">[Talk](https://slideslive.com/38959028/on-proximal-policy-optimizations-heavytailed-gradients?ref=speaker-37449-latest)</span> / <span style="color:blue">[Poster](https://drive.google.com/file/d/1U2GxKvBqEC32vY-DZxnzHT80rj8fePqr/view?usp=sharing)</span> / <span style="color:blue">[Summary](javascript:toggleblock('garg2021_PPO_abs'))</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2021_PPO_bib'))</span>
<p><i id="garg2021_PPO_abs" style="display: none;">We empirically characterized PPO’s gradients, demonstrating that they become more heavy-tailed as training proceeds. We examined issues due to heavy-tailed nature of gradients and show that PPO clipping heuristics offset heavy-tailedness in gradients. </i></p>
<bibtext xml:space="preserve" id="garg2021_PPO_bib" style="display: none;">
@article{garg2021PPO, <br>
 title={ On Proximal Policy Optimization’s Heavy-tailed Gradients } <br>
 author={Garg, Saurabh and Zhanson, Joshua and Parisotto, Emilio and Prasad, Adarsh and Kolter, J Zico and Balakrishnan, Sivaraman and Lipton, Zachary C and Salakhutdinov, Ruslan and Ravikumar, Pradeep}, <br>
 year={2021}, <br>
 journal={arXiv preprint arXiv:2102.10264} <br>
 url={https://arxiv.org/abs/2102.10264}, <br>
 note={Short Talk at ICML 2021}<br>
}
</bibtext>

**A Unified View of Label Shift Estimation**   
Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, Zachary Lipton  
Advances in Neural Information Processing Systems (NeurIPS), 2020        
<span style="color:red">Contributed Talk</span> at ICML Workshop on Uncertainty in Deep Learning, 2020    
<span style="color:blue">[Paper](https://arxiv.org/abs/2003.07554)</span> / <span style="color:blue">[Talk](https://slideslive.com/38930578/a-unified-view-of-label-shift-estimation?ref=speaker-37449-latest)</span> / <span style="color:blue">[Poster](https://drive.google.com/file/d/13hpynIYM69nSRqj-7CHdvEdG7amC9phy/view?usp=sharing)</span> / <span style="color:blue">[Summary](javascript:toggleblock('garg2021_labelshift_abs'))</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2021_labelshift_bib'))</span>
<p><i id="garg2021_labelshift_abs" style="display: none;">We provide a unified framework relating techniques that use off-the-shelf predictors for label shift estimation. We argue that these methods all employ calibration, either explicitly or implicitly, differing only in the choice of calibration method and their optimization objective. </i></p>
<bibtext xml:space="preserve" id="garg2021_labelshift_bib" style="display: none;">
@article{garg2020labelshift, <br>
 title={ A Unified View of Label Shift Estimation } <br>
 author={Garg, Saurabh and Wu, Yifan and Balakrishnan, Sivaraman and Lipton, Zachary}, <br>
 year={2020}, <br>
 journal={arXiv preprint arXiv:2003.07554} <br>
 url={https://arxiv.org/abs/2003.07554}, <br>
 note={NeurIPS 2020}<br>
}
</bibtext>



**Neural Architecture for Question Answering Using a Knowledge Graph and Web Corpus**  
Uma Sawant, Saurabh Garg, Soumen Chakrabarti, Ganesh Ramakrishnan  
Information Retrieval Journal, 2019      
<span style="color:red">Invited Oral</span> at European Conference on Information Retrieval (ECIR), 2020    
<span style="color:blue">[Paper](https://arxiv.org/abs/1706.00973)</span> / <span style="color:blue">[Talk](https://youtu.be/cVZ3Qj8sJCk?t=24540)</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2019_QA_bib'))</span>
<bibtext xml:space="preserve" id="garg2019_QA_bib" style="display: none;">
@article{sawant2019neural,
  title={Neural architecture for question answering using a knowledge graph and web corpus}, <br>
  author={Sawant, Uma and Garg, Saurabh and Chakrabarti, Soumen and Ramakrishnan, Ganesh}, <br>
  journal={Information Retrieval Journal}, <br>
  volume={22}, <br>
  number={3}, <br>
  pages={324--349}, <br>
  year={2019}, <br>
  publisher={Springer}<br>
}
</bibtext>

**Estimating Uncertainty in MRF-based Image Segmentation: An Exact-MCMC Approach**  
Suyash Awate, Saurabh Garg, Rohit Jena  
Medical Image Analysis Journal, 2019      
<span style="color:blue">[Paper](https://doi.org/10.1016/j.media.2019.04.014)</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2019_MEDIA_bib'))</span>
<bibtext xml:space="preserve" id="garg2019_MEDIA_bib" style="display: none;">
@article{awate2019estimating,<br>
  title={Estimating uncertainty in MRF-based image segmentation: A perfect-MCMC approach},<br>
  author={Awate, Suyash P and Garg, Saurabh and Jena, Rohit},<br>
  journal={Medical image analysis},<br>
  volume={55},<br>
  pages={181--196},<br>
  year={2019},<br>
  publisher={Elsevier}<br>
}
</bibtext>

**Code-Switched Language models using Dual RNNs and Same-Source Pretraining**  
Saurabh Garg\*, Tanmay Parekh\*, Preethi Jyothi (\*joint first authors)  
*Awarded EMNLP Non-Student Travel Grant*  
Proceedings of Empirical Methods in Natural Language Processing (EMNLP), 2018     
<span style="color:blue">[Paper](http://aclweb.org/anthology/D18-1346)</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2018_EMNLP_bib'))</span>
<bibtext xml:space="preserve" id="garg2018_EMNLPA_bib" style="display: none;">
@inproceedings{garg2018code,<br>
  title={Code-switched Language Models Using Dual RNNs and Same-Source Pretraining},<br>
  author={Garg, Saurabh and Parekh, Tanmay and Jyothi, Preethi},<br>
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},<br>
  pages={3078--3083},<br>
  year={2018}<br>
}
</bibtext>

**Uncertainty Estimation in Segmentation with Perfect MCMC Sampling in Bayesian MRFs**  
Saurabh Garg, Suyash Awate  
Proceedings of Medical Image Computing & Computer Assisted Intervention (MICCAI), 2019   
<span style="color:blue">[Paper](https://link.springer.com/chapter/10.1007/978-3-030-00928-1_76)</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2018_MICCAI_bib'))</span>
<bibtext xml:space="preserve" id="garg2018_MICCAI_bib" style="display: none;">
@inproceedings{garg2018perfect,<br>
  title={Perfect MCMC sampling in Bayesian MRFs for uncertainty estimation in segmentation},<br>
  author={Garg, Saurabh and Awate, Suyash P},<br>
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},<br>
  pages={673--681},<br>
  year={2018},<br>
  organization={Springer}<br>
}
</bibtext>

**Dual Language Models for Code Mixed Speech Recognition**  
Saurabh Garg, Tanmay Parekh, Preethi Jyothi   
*Awarded ISCA Student Travel Grant*  
Proceedings of Interspeech 2018 (19th Annual Conference of ISCA)     
<span style="color:blue">[Paper](https://www.semanticscholar.org/paper/Dual-Language-Models-for-Code-Switched-Speech-Garg-Parekh/5c0371c3e34722f0fbdf5669c8e5361fac60bbcd)</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2018_Interspeech_bib'))</span>
<bibtext xml:space="preserve" id="garg2018_Interspeech_bib" style="display: none;">
@article{garg2017dual,<br>
  title={Dual language models for code switched speech recognition},<br>
  author={Garg, Saurabh and Parekh, Tanmay and Jyothi, Preethi},<br>
  journal={arXiv preprint arXiv:1711.01048},<br>
  year={2017}<br>
}
</bibtext>

