---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
urlcolor: blue
---

You can also find publications on my <span style="color:blue">[Google Scholar](https://scholar.google.com/citations?user=SAnJ1hIAAAAJ&hl=en")</span> profile. 

**Leveraging Unlabeled Data to Predict Out-of-Distribution Performance**   
Saurabh Garg, Sivaraman Balakrishnan, Zachary Lipton, Behnam Neyshabur, Hanie Sedghi  
NeurIPS Workshop on Distribution Shift (DistShift), 2021   
Under Submission  


**Mixture Proportion Estimation and PU Learning: A Modern Approach**  
Saurabh Garg, Yifan Wu, Alex Smola, Sivaraman Balakrishnan, Zachary Lipton  
<span style="color:red">Spotlight</span> at Advances in Neural Information Processing Systems (NeurIPS), 2021       
ICML Workshop on Uncertainty in Deep Learning, 2021   
<span style="color:blue">[Paper](https://arxiv.org/abs/2111.00980)</span> / <span style="color:blue">[Code](https://github.com/acmi-lab/PU_learning)</span> / <span style="color:blue">[Talk](https://recorder-v3.slideslive.com/?share=52493&s=dafe9010-00e1-4082-b224-0391b356ccbb)</span> / <span style="color:blue">[Poster](https://drive.google.com/file/d/1liowCWEHxMZH2Ag5ozaJfAG58RCwOh86/view?usp=sharing)</span> / <span style="color:blue">[Summary](javascript:toggleblock('garg2021_PU_learning_abs'))</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2021_PU_learning_bib'))</span>
<p><i id="garg2021_PU_learning_abs" style="display: none;">Given only Positive (P) and Unlabeled (U) data, containing both P and Negative (N) samples, we propose new approaches to estimate fraction of P in U and learn P vs N classifier. </i></p>
<bibtext xml:space="preserve" id="garg2021_PU_learning_bib" style="display: none;">
@article{garg2021PUlearning, <br> 
 title={Mixture Proportion Estimation and PU Learning: A Modern Approach} <br>
 author={Garg, Saurabh and Wu, Yifan and Smola, Alex and Balakrishnan, Sivaraman and Lipton, Zachary}, <br>
 year={2021}, <br>
 journal={arXiv preprint arXiv:2111.00980} <br>
 url={https://arxiv.org/abs/2111.00980}, <br>
 note={Spotlight at NeurIPS 2021}
}
</bibtext>

**RATT: Leveraging Unlabeled Data to Guarantee Generalization**   
Saurabh Garg, Sivaraman Balakrishnan, Zico Kolter, Zachary Lipton   
<span style="color:red">Long Talk</span> at International Conference on Machine Learning (ICML), 2021   
ICLR Workshop on RobustML, 2021   
<span style="color:blue">[Paper](https://arxiv.org/abs/2105.00303)</span> / <span style="color:blue">[Code](https://github.com/acmi-lab/RATT_generalization_bound)</span> / <span style="color:blue">[Talk](https://slideslive.com/38958661/ratt-leveraging-unlabeled-data-to-guarantee-generalization?ref=speaker-37449-latest)</span> / <span style="color:blue">[Poster](https://drive.google.com/file/d/1H25csKq622EDMtw2en-aDQxqNcP1Mcdg/view?usp=sharing)</span> / <span style="color:blue">[Summary](javascript:toggleblock('garg2021_RATT_abs'))</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2021_RATT_bib'))</span>
<p><i id="garg2021_RATT_abs" style="display: none;">We introduce a method that leverages unlabeled data to produce generalization bound.  When a trained model fits clean (training) data well but randomly labeled (training) data (added in) poorly, we show that its generalization (to the population) is guaranteed.</i></p>
<bibtext xml:space="preserve" id="garg2021_RATT_bib" style="display: none;">
@article{garg2021RATT, <br>
 title={ {RATT}: Leveraging Unlabeled Data to Guarantee Generalization } <br>
 author={Garg, Saurabh and Balakrishnan, Sivaraman and Kolter, Zico and Lipton, Zachary}, <br>
 year={2021}, <br>
 journal={arXiv preprint arXiv:2105.00303} <br>
 url={https://arxiv.org/abs/2105.00303}, <br>
 note={Long Talk at ICML 2021}
}
</bibtext>


**On Proximal Policy Optimization's Heavy-Tailed Gradients**   
Saurabh Garg, Joshua Zhanson, Emilio Parisotto, Adarsh Prasad, Zico Kolter, Zachary Lipton, Sivaraman Balakrishnan, Ruslan Salakhutdinov, Pradeep Ravikumar    
Short talk at International Conference on Machine Learning (ICML), 2021   
ICLR Workshop on Science and Engineering of Deep Learning, 2021    
<span style="color:blue">[Paper](https://arxiv.org/pdf/2102.10264.pdf)</span> | <span style="color:blue">[Talk](https://drive.google.com/file/d/1Uvcuqbcv9w2NQNSVoOdoLsDcyf2FpBc3/view?usp=sharing)</span> | <span style="color:blue">[Poster](https://drive.google.com/file/d/1U2GxKvBqEC32vY-DZxnzHT80rj8fePqr/view?usp=sharing)</span> | <span style="color:blue">[Summary]()</span> | <span style="color:blue">[Citation]()</span>
<span style="color:blue">[Paper](https://arxiv.org/abs/2105.00303)</span> / <span style="color:blue">[Code](https://github.com/acmi-lab/RATT_generalization_bound)</span> / <span style="color:blue">[Talk](https://slideslive.com/38958661/ratt-leveraging-unlabeled-data-to-guarantee-generalization?ref=speaker-37449-latest)</span> / <span style="color:blue">[Poster](https://drive.google.com/file/d/1H25csKq622EDMtw2en-aDQxqNcP1Mcdg/view?usp=sharing)</span> / <span style="color:blue">[Summary](javascript:toggleblock('garg2021_RATT_abs'))</span> / <span style="color:blue">[Bibtex](javascript:toggleblock('garg2021_RATT_bib'))</span>
<p><i id="garg2021_RATT_abs" style="display: none;">We introduce a method that leverages unlabeled data to produce generalization bound.  When a trained model fits clean (training) data well but randomly labeled (training) data (added in) poorly, we show that its generalization (to the population) is guaranteed.</i></p>
<bibtext xml:space="preserve" id="garg2021_RATT_bib" style="display: none;">
@article{garg2021RATT, <br>
 title={ {RATT}: Leveraging Unlabeled Data to Guarantee Generalization } <br>
 author={Garg, Saurabh and Balakrishnan, Sivaraman and Kolter, Zico and Lipton, Zachary}, <br>
 year={2021}, <br>
 journal={arXiv preprint arXiv:2105.00303} <br>
 url={https://arxiv.org/abs/2105.00303}, <br>
 note={Long Talk at ICML 2021}
}
</bibtext>

**A Unified View of Label Shift Estimation**   
Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, Zachary Lipton  
Advances in Neural Information Processing Systems (NeurIPS), 2020 \[[<span style="color:blue">Paper</span>](https://arxiv.org/abs/2003.07554)\] \[[<span style="color:blue">Poster</span>](https://drive.google.com/file/d/13hpynIYM69nSRqj-7CHdvEdG7amC9phy/view?usp=sharing)\]      
<span style="color:red">Contributed Talk</span> at ICML Workshop on Uncertainty in Deep Learning, 2020 \[[<span style="color:blue">Talk</span>](https://slideslive.com/38930578/a-unified-view-of-label-shift-estimation?ref=speaker-37449-latest)\]    
<span style="color:blue">[Paper](https://arxiv.org/abs/2003.07554)</span> | <span style="color:blue">[Talk](https://drive.google.com/file/d/1Uvcuqbcv9w2NQNSVoOdoLsDcyf2FpBc3/view?usp=sharing)</span> | <span style="color:blue">[Poster](https://drive.google.com/file/d/1U2GxKvBqEC32vY-DZxnzHT80rj8fePqr/view?usp=sharing)</span> | <span style="color:blue">[Summary]()</span> | <span style="color:blue">[Citation]()</span>

**Neural Architecture for Question Answering Using a Knowledge Graph and Web Corpus**  
Uma Sawant, Saurabh Garg, Soumen Chakrabarti, Ganesh Ramakrishnan  
Information Retrieval Journal, 2019 \[[<span style="color:blue">Paper</span>](https://arxiv.org/abs/1706.00973)\]  
<span style="color:red">Invited Oral</span> at European Conference on Information Retrieval (ECIR), 2020 \[[<span style="color:blue">Talk</span>](https://youtu.be/cVZ3Qj8sJCk?t=24540)\]


**Estimating Uncertainty in MRF-based Image Segmentation: An Exact-MCMC Approach**  
Suyash Awate, Saurabh Garg, Rohit Jena  
Medical Image Analysis Journal, 2019 \[[<span style="color:blue">Paper</span>](https://doi.org/10.1016/j.media.2019.04.014)\]


**Code-Switched Language models using Dual RNNs and Same-Source Pretraining**  
Saurabh Garg\*, Tanmay Parekh\*, Preethi Jyothi (\*joint first authors)  
*Awarded EMNLP Non-Student Travel Grant*  
Proceedings of Empirical Methods in Natural Language Processing (EMNLP), 2018 \[[<span style="color:blue">Paper</span>](http://aclweb.org/anthology/D18-1346)\]  


**Uncertainty Estimation in Segmentation with Perfect MCMC Sampling in Bayesian MRFs**  
Saurabh Garg, Suyash Awate  
Proceedings of Medical Image Computing & Computer Assisted Intervention (MICCAI), 2019 \[[<span style="color:blue">Paper</span>](https://link.springer.com/chapter/10.1007/978-3-030-00928-1_76)\]


**Dual Language Models for Code Mixed Speech Recognition**  
Saurabh Garg, Tanmay Parekh, Preethi Jyothi   
*Awarded ISCA Student Travel Grant*  
Proceedings of Interspeech 2018 (19th Annual Conference of ISCA) \[[<span style="color:blue">Paper</span>](https://www.semanticscholar.org/paper/Dual-Language-Models-for-Code-Switched-Speech-Garg-Parekh/5c0371c3e34722f0fbdf5669c8e5361fac60bbcd)\]


